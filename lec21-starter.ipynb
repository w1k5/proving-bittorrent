{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f348d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b95782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backdoor_adjustment(Y, A, Z, data):\n",
    "    \"\"\"\n",
    "    Compute the average causal effect E[Y(A=1)] - E[Y(A=0)] via backdoor adjustment\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    Y: string corresponding variable name of the outcome\n",
    "    A: string corresponding variable name\n",
    "    Z: list of variable names to adjust for\n",
    "    data: pandas dataframe\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    ACE: float corresponding to the causal effect\n",
    "    \"\"\"\n",
    "    \n",
    "    formula = Y + \"~\" + A\n",
    "    if len(Z) > 0:\n",
    "        formula += \" + \" + \"+\".join(Z)\n",
    "    \n",
    "    model = sm.GLM.from_formula(formula=formula, data=data, family=sm.families.Gaussian()).fit()\n",
    "    data_A0 = data.copy()\n",
    "    data_A1 = data.copy()\n",
    "    data_A0[A] = 0\n",
    "    data_A1[A] = 1\n",
    "    return(np.mean(model.predict(data_A1)) - np.mean(model.predict(data_A0)))\n",
    "\n",
    "def get_numpy_matrix(data, variables):\n",
    "    \"\"\"\n",
    "    Takes a pandas dataframe and a list of variable names, and returns\n",
    "    just the raw matrix for those specific variables\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    matrix = data[variables].to_numpy()\n",
    "\n",
    "    # if there's only one variable, ensure we return a matrix with one column\n",
    "    # rather than just a column vector\n",
    "    if len(variables) == 1:\n",
    "        return matrix.reshape(len(data),)\n",
    "    return matrix\n",
    "    \n",
    "    \n",
    "def backdoor_ML(Y, A, Z, data):\n",
    "    \"\"\"\n",
    "    Compute the average causal effect E[Y(A=1)] - E[Y(A=0)] via backdoor adjustment with random forests.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    Y: string corresponding variable name of the outcome\n",
    "    A: string corresponding variable name\n",
    "    Z: list of variable names to adjust for\n",
    "    data: pandas dataframe\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    ACE: float corresponding to the causal effect\n",
    "    \"\"\"\n",
    "    \n",
    "    # some starter code to get the matrix for doing the regression\n",
    "    # in the right shape\n",
    "    outcome = get_numpy_matrix(data, [Y])\n",
    "    predictors = get_numpy_matrix(data, [A] + Z)\n",
    "\n",
    "    # use a RandomForestRegressor with bootstrap=False\n",
    "\n",
    "    # get predictions using datasets where A=1 and A=0\n",
    "\n",
    "    return 1\n",
    "\n",
    "def aipw_ML(Y, A, Z, data):\n",
    "    \"\"\"\n",
    "    Compute the average causal effect E[Y(A=1)] - E[Y(A=0)] via augmented IPW with random forests.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    Y: string corresponding variable name of the outcome\n",
    "    A: string corresponding variable name\n",
    "    Z: list of variable names to adjust for\n",
    "    data: pandas dataframe\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    ACE: float corresponding to the causal effect\n",
    "    \"\"\"\n",
    "    \n",
    "    # some starter code to get the matrix for doing the regressions\n",
    "    # in the right shape\n",
    "    treatment = get_numpy_matrix(data, [A])\n",
    "    predictors_a = get_numpy_matrix(data, Z)\n",
    "    \n",
    "    # use a RandomForestClassifier with bootstrap=False for fitting a model for p(A|Z)\n",
    "    \n",
    "    # use p(A|Z) model to predict propensity scores and get IPW weights\n",
    "    \n",
    "    \n",
    "    # fit a RandomForestRegressor model for E[Y|A, Z, W] where W are the IPW weights\n",
    "    \n",
    "    # get predictions using datasets where A=1 and A=0\n",
    "\n",
    "    return 1\n",
    "\n",
    "def backdoor_mean(Y, A, Z, value, data):\n",
    "    \"\"\"\n",
    "    Compute the counterfactual mean E[Y(a)] for a given value of a via backdoor adjustment\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    Y: string corresponding variable name of the outcome\n",
    "    A: string corresponding variable name\n",
    "    Z: list of variable names to adjust for\n",
    "    value: float corresponding value to set A to\n",
    "    data: pandas dataframe\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    ACE: float corresponding to the causal effect\n",
    "    \"\"\"\n",
    "    \n",
    "    formula = Y + \"~\" + A\n",
    "    if len(Z) > 0:\n",
    "        formula += \" + \" + \"+\".join(Z)\n",
    "    \n",
    "    model = sm.GLM.from_formula(formula=formula, data=data, family=sm.families.Gaussian()).fit()\n",
    "    data_a = data.copy()\n",
    "    data_a[A] = value\n",
    "    return np.mean(model.predict(data_a))\n",
    "    \n",
    "def compute_confidence_intervals(Y, A, Z, data, method_name, num_bootstraps=100, alpha=0.05, value=None):\n",
    "    \"\"\"\n",
    "    Compute confidence intervals for backdoor adjustment via bootstrap\n",
    "    \n",
    "    Returns tuple (q_low, q_up) for the lower and upper quantiles of the confidence interval.\n",
    "    \"\"\"\n",
    "    \n",
    "    Ql = alpha/2\n",
    "    Qu = 1 - alpha/2\n",
    "    estimates = []\n",
    "    \n",
    "    for i in range(num_bootstraps):\n",
    "        \n",
    "        # resample the data with replacement\n",
    "        data_sampled = data.sample(len(data), replace=True)\n",
    "        data_sampled.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # add estimate from resampled data\n",
    "        if method_name == \"backdoor\":\n",
    "            estimates.append(backdoor_adjustment(Y, A, Z, data_sampled))\n",
    "        \n",
    "        elif method_name == \"backdoor_ML\":\n",
    "            estimates.append(backdoor_ML(Y, A, Z, data_sampled))\n",
    "            \n",
    "        elif method_name == \"aipw_ML\":\n",
    "            estimates.append(aipw_ML(Y, A, Z, data_sampled))\n",
    "            \n",
    "        elif method_name == \"backdoor_mean\":\n",
    "            estimates.append(backdoor_mean(Y, A, Z, value, data_sampled))\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid method\")\n",
    "            estimates.append(1)\n",
    "\n",
    "    # calculate the quantiles\n",
    "    quantiles = np.quantile(estimates, q=[Ql, Qu])\n",
    "    q_low = quantiles[0]\n",
    "    q_up = quantiles[1]\n",
    "    \n",
    "    return q_low, q_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a05a0a",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "The code block below contains code to generate data from a simple model where $A \\leftarrow Z \\rightarrow Y$ and $A\\rightarrow Y.$ That is, $Z=\\{Z_1, Z_2, Z_3\\}$ is a valid adjustment set for the effect of $A$ on $Y.$ However, the relations between these variables are non-linear and so backdoor adjustment/IPW with linear models is expected to produce biased estimates. The true average causal effect is 2. We will see if we are able to recover this causal effect using machine learning.\n",
    "\n",
    "1. Implement the `backdoor_ML` function that uses a `RandomForestRegressor` to fit $\\mathbb{E}[Y\\mid A, Z].$ How do the estimates from this method compare to estimates obtained by using linear models for $n=200, 2000, 20000$ samples? What does this say about plug-in bias?\n",
    "\n",
    "\n",
    "2. Implement the `aipw_ML` function that uses a `RandomForestClassifier` for the propensity score model $p(A\\mid Z)$ and a `RandomForestRegressor` for the outcome regression $\\mathbb{E}[Y \\mid A, Z].$ Evaluate the causal effect using the *aipw\\_ML* method at the same sample sizes as before. Does it help protect against plug-in bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data generating process\n",
    "size = 200\n",
    "Z1 = np.random.normal(0, 1, size)\n",
    "Z2 = np.random.binomial(1, 0.5, size)\n",
    "Z3 = np.random.uniform(0, 1, size)\n",
    "A = np.random.binomial(1, expit(Z1*Z3 + Z1*Z2 + Z1**2), size)\n",
    "Y = 1.5 + 2.5*Z1*Z2*Z3 + Z1**2 + Z2**2 + 2*A + np.random.normal(0, 1, size)\n",
    "data = pd.DataFrame({\"Y\": Y, \"A\": A, \"Z1\": Z1, \"Z2\": Z2, \"Z3\": Z3})\n",
    "\n",
    "# point estimate and confidence intervals using linear models in backdoor adjustment\n",
    "point_estimate = backdoor_adjustment(\"Y\", \"A\", [\"Z1\", \"Z2\", \"Z3\"], data)\n",
    "cis = compute_confidence_intervals(\"Y\", \"A\", [\"Z1\", \"Z2\", \"Z3\"], data, method_name=\"backdoor\")\n",
    "print(point_estimate, cis)\n",
    "np.mean(A)\n",
    "\n",
    "# point estimate using machine learning in backdoor adjustment\n",
    "point_estimate = backdoor_ML(\"Y\", \"A\", [\"Z1\", \"Z2\", \"Z3\"], data)\n",
    "cis = compute_confidence_intervals(\"Y\", \"A\", [\"Z1\", \"Z2\", \"Z3\"], data, method_name=\"backdoor_ML\")\n",
    "print(point_estimate, cis)\n",
    "\n",
    "# point estimate using machine learning for reweighted backdoor adjustment\n",
    "point_estimate = aipw_ML(\"Y\", \"A\", [\"Z1\", \"Z2\", \"Z3\"], data)\n",
    "cis = compute_confidence_intervals(\"Y\", \"A\", [\"Z1\", \"Z2\", \"Z3\"], data, method_name=\"aipw_ML\")\n",
    "print(point_estimate, cis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5689b0",
   "metadata": {},
   "source": [
    "## Dealing with continuous treatment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data generating process\n",
    "size = 200\n",
    "Z = np.random.uniform(0, 1, size)\n",
    "A = 1.2*Z + np.random.uniform(0, 2, size)\n",
    "Y = 1.5 + 2.5*Z + 2*A + A**4 + np.random.normal(0, 1, size)\n",
    "data = pd.DataFrame({\"Y\": Y, \"A\": A, \"Z\": Z})\n",
    "\n",
    "point_estimates = []\n",
    "values_a = list(np.arange(0, 2, 0.2))\n",
    "for value in values_a:\n",
    "    point_estimates.append(backdoor_mean(\"Y\", \"A\", [\"Z\", \"np.power(A, 4)\"], value, data))\n",
    "\n",
    "ci_lower = []\n",
    "ci_upper = []\n",
    "for value in values_a:\n",
    "    ci = compute_confidence_intervals(\"Y\", \"A\", [\"Z\", \"np.power(A, 4)\"],\n",
    "                                      data, method_name=\"backdoor_mean\", value=value)\n",
    "    ci_lower.append(ci[0])\n",
    "    ci_upper.append(ci[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "point_estimates = np.array(point_estimates)\n",
    "ci_lower, ci_upper = np.array(ci_lower), np.array(ci_upper)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(values_a, point_estimates)\n",
    "ax.fill_between(values_a, ci_lower, ci_upper, color='b', alpha=.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
